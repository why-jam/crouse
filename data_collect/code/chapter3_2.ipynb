{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "https://www.xtu.edu.cn/bwcxljsm/\n",
      "https://portal2020.xtu.edu.cn\n",
      "http://portal.xtu.edu.cn\n",
      "http://nic.xtu.edu.cn/\n",
      "/ks/\n",
      "/xy/\n",
      "https://en.xtu.edu.cn/\n",
      "http://fuwu.xtu.edu.cn/web/index.jsp\n",
      "http://weibo.com/u/5725856399?refer_flag=1005055010_&is_hot=1\n",
      "http://www.xtu.edu.cn/xysh/xyfg/3dxy/\n",
      "http://zwxxg.xtu.edu.cn/\n",
      "http://lib.xtu.edu.cn/\n",
      "http://zp.xtu.edu.cn/\n",
      "http://www.sky31.com/\n",
      "http://fund.xtu.edu.cn/\n",
      "/zbgg/\n",
      "#\n",
      "http://mail.xtu.edu.cn/\n",
      "https://mail.smail.xtu.edu.cn/\n",
      "http://zwxxg.xtu.edu.cn/\n",
      "http://dzb.xtu.edu.cn/meeting/\n",
      "http://xxgk.xtu.edu.cn/\n",
      "http://nic.xtu.edu.cn\n",
      "http://xdgjzx.xtu.edu.cn\n",
      "http://kczx.xtu.edu.cn\n",
      "http://jwxt.xtu.edu.cn/jsxsd\n",
      "http://zwfw-new.hunan.gov.cn/?tdsourcetag=s_pctim_aiomsg\n",
      "http://www.hnedu.gov.cn/index.html\n",
      "http://www.hnedu.cn/\n",
      "http://www.moe.gov.cn/\n",
      "http://www.worlduc.com/\n",
      "https://en.xtu.edu.cn/\n",
      "https://www.xtu.edu.cn/bwcxljsm/\n",
      "https://portal2020.xtu.edu.cn\n",
      "http://portal.xtu.edu.cn\n",
      "http://nic.xtu.edu.cn/\n",
      "/ks\n",
      "/xy\n",
      "/\n",
      "/xxgk/\n",
      "/jgsz/department/\n",
      "/szdw/szgk/\n",
      "/jyjx/xkjs/syl/\n",
      "/kxyj/zrkx/\n",
      "/zsjy/bkszs/\n",
      "/hzjl/gjhz/jianj/\n",
      "/xysh/\n",
      "/xxgk/xxjj/\n",
      "/xxgk/fzyj/\n",
      "/xxgk/xdjs/xming/\n",
      "/xxgk/xxzc/\n",
      "/xxgk/xrld/\n",
      "/xxgk/lrld/\n",
      "/xxgk/xswyh/\n",
      "/xxgk/dsh/\n",
      "/jgsz/department/\n",
      "/jgsz/party/\n",
      "/jgsz/organization/\n",
      "/jgsz/directly/\n",
      "/szdw/szgk/\n",
      "/szdw/tcrc/\n",
      "http://zp.xtu.edu.cn/\n",
      "/jyjx/xkjs/syl/\n",
      "/jyjx/bksjy/\n",
      "/jyjx/yjsjy/\n",
      "/jyjx/lxsjy/lxxd/\n",
      "/jyjx/jxjy/\n",
      "/jyjx/kczx/\n",
      "/jyjx/xzfc/\n",
      "/jyjx/cxcy/\n",
      "/kxyj/zrkx/\n",
      "/kxyj/shkx/\n",
      "/kxyj/kyjg/zkl/\n",
      "/kxyj/kycg/zkl/\n",
      "/kxyj/kyxm/zkl/\n",
      "/kxyj/xsqk/\n",
      "/kxyj/cxyhz/zrkxcxy/\n",
      "/kxyj/xszl/kx/\n",
      "/kxyj/keyantuandui/\n",
      "/zsjy/bkszs/\n",
      "/zsjy/yjszs/\n",
      "/zsjy/lxszs/\n",
      "http://www.xtu.edu.cn/zsjy/jxjyzs/chenren/2016-06-28/2267.html\n",
      "/zsjy/jiuyexinxiwang/\n",
      "/hzjl/gjhz/jianj/\n",
      "/hzjl/xjhz/\n",
      "/hzjl/xdhz/\n",
      "/hzjl/xqhz/\n",
      "/xysh/xywh/xsst/\n",
      "/xysh/shss/cyfw/\n",
      "/xysh/hyss/\n",
      "/xysh/tyyd/\n",
      "/xysh/ggfw/tsg/\n",
      "/xysh/xyfg/xydt/\n",
      "/xysh/xyjt/\n",
      "http://zwxxg.xtu.edu.cn/\n",
      "http://xxgk.xtu.edu.cn/\n",
      "http://lib.xtu.edu.cn/\n",
      "http://zp.xtu.edu.cn/\n",
      "http://fund.xtu.edu.cn/\n",
      "http://www.sky31.com/\n",
      "/zbgg/\n",
      "#\n",
      "http://mail.xtu.edu.cn/\n",
      "https://mail.smail.xtu.edu.cn/\n",
      "http://zwxxg.xtu.edu.cn/\n",
      "http://dzb.xtu.edu.cn/meeting/\n",
      "http://xxgk.xtu.edu.cn/\n",
      "http://nic.xtu.edu.cn\n",
      "http://xdgjzx.xtu.edu.cn\n",
      "http://kczx.xtu.edu.cn\n",
      "http://jwxt.xtu.edu.cn/jsxsd\n",
      "http://zwfw-new.hunan.gov.cn/?tdsourcetag=s_pctim_aiomsg\n",
      "http://www.hnedu.gov.cn/index.html\n",
      "http://www.hnedu.cn/\n",
      "http://www.moe.gov.cn/\n",
      "http://www.worlduc.com/\n",
      "http://news.xtu.edu.cn/\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12186.html\n",
      "kxyj/keyantuandui\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12185.html\n",
      "kxyj/keyantuandui\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12154.html\n",
      "kxyj/keyantuandui\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12149.html\n",
      "kxyj/keyantuandui\n",
      "/xdxw/xnxw/\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12263.html\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12261.html\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12260.html\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12258.html\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12257.html\n",
      "http://news.xtu.edu.cn/html/zonghexw/show_12256.html\n",
      "http://news.xtu.edu.cn/html/meitixd\n",
      "http://news.xtu.edu.cn/html/meitigz/show_12262.html\n",
      "http://news.xtu.edu.cn/html/meitixd/show_12259.html\n",
      "http://news.xtu.edu.cn/html/meitixd/show_12253.html\n",
      "http://news.xtu.edu.cn/index.jsp?cc=cindex&cd=news&ac=view&cachehtm=true&preview=true&id=12251\n",
      "http://news.xtu.edu.cn/html/meitixd/show_12247.html\n",
      "http://news.xtu.edu.cn/html/meitixd/show_12239.html\n",
      "https://portal2020.xtu.edu.cn\n",
      "http://fuwu.xtu.edu.cn/web/index.jsp\n",
      "http://weibo.com/u/5725856399?refer_flag=1005055010_&is_hot=1\n",
      "http://www.xtu.edu.cn/xysh/xyfg/3dxy/\n",
      "/gonggao/\n",
      "/gonggao/wlzx/2020-03-13/7853.html\n",
      "/gonggao/gonggao/2020-02-12/7822.html\n",
      "/gonggao/gonggao/2020-01-16/7799.html\n",
      "/zxzt\n",
      "https://xtu.campusphere.net/wec-amp-boya/pc/index.html?login=0\n",
      "https://www.xtu.edu.cn/bwcxljsm/\n",
      "/xysh/xywh/xsjz/\n",
      "/xysh/xywh/xsjz/jxgcxy/2019-12-11/7580.html\n",
      "/xysh/xywh/xsjz/jxgcxy/2019-12-11/7579.html\n",
      "http://www.beian.miit.gov.cn\n",
      "http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=43030202001058\n",
      "http://bszs.conac.cn/sitename?method=show&id=279AAB28EB280ACCE053022819AC0088\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "s = requests.session()\n",
    "s.headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:63.0) Gecko/20100101 Firefox/63.0',\n",
    "    'Accept': 'text/html, application/xhtml+xml, application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language':'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "#page = s.get('https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895?fr=aladdin')\n",
    "page = s.get('https://www.xtu.edu.cn/')\n",
    "#print(page.text)\n",
    "html = etree.HTML(page.text)\n",
    "hrefs = html.xpath(\"//a/@href\")\n",
    "for href in hrefs:\n",
    "    print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先我们来定义一段 HTML 文档，接下来的讲解都将以此为基础。\n",
    "html = \"\"\"\n",
    "<html><head><meta charset=\"UTF-8\"/>\n",
    "<title>HTML 测试页面</title>\n",
    "</head>\n",
    " <body>换行1\n",
    "  <div id=\"div1\">\n",
    "  <h1>一级标题</h1>\n",
    "  <a href=\"http://www.example.com/example1.html\" class=\"link1\">链接1</a>\n",
    "  <a href=\"example2.html\" class=\"link2\">链接2</a></div>换行2\n",
    "  <div id=\"div2\"><h2>二级标题</h2><ul><li value=\"1\">项目1</li><li>项目2</li></ul></div>换行3\n",
    "  <comment><!--This is comment，这是注释--></comment>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "True HTML 测试页面\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "print (soup.original_encoding) # 输出 BeautifulSoup 猜测出的网页的编码。\n",
    "print (isinstance(soup.title.string, str), soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码中 soup 就是 BeautifulSoup 将 HTML 转成的 DOM 树。需要注意的是html 的源码是不完整的，缺少了 <html> 和 <body> 对应的关闭标签，但 BeautifulSoup 仍然能正确解析，这就是所谓的容错性。我们可以调用soup.prettify查看格式化之后的 HTML 源码。\n",
    "\n",
    "使用 BeautifulSoup，我们不需要过多地考虑编码的问题。BeautifulSoup 会自动检测网页编码，并将文档转换为 str（在上面的代码中可以看到页面的标题 \"HTML 测试页面\" 在内存中是以 str 形式存在的），然后再存到 DOM 树中。无论原始网页使用何种编码，在将 DOM 树中的节点转换成字符串输出时（如soup.prettify），BeautifulSoup 统一采用 UTF-8编码。这也告诫我们，尽量采用 UTF-8 编码，避免在编码上浪费过多的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节点类型\n",
    "\n",
    "上面代码中 soup.title.string 的含义是：从 soup 这个 DOM 树中选择第一个标签名为 title 的节点，然后选择这个标签中的内容。 树中每个节点都是一个对象，这些节点对象可以归纳为3种：Tag，NavigableString，Comment。它们的含义如下所示：\n",
    "\n",
    "    Tag ：Tag 就是 HTML 中的标签，表现在 DOM 树中就是一个节点。一个 Tag 可以包含其它 Tag 或 NavigableString。这是我们主要操作的对象。\n",
    "    NavigableString ：BeautifulSoup 用 NavigableString 类来包装 Tag 中的字符串，是一个特殊的节点，没有子节点。这是我们要提取的对象。\n",
    "    Comment ：Comment 是NavigableString的子类，表示HTML 文件中的注释。通常我们都不关注 HTML 中的注释。\n",
    "    BeautifulSoup ：不是节点类型，是整个 DOM 树的类型，但BeautifulSoup中提供了很多快捷方法，通常情况下我们可以把它当做一个 Tag 来使用。如上面代码中的 soup 就是一个 BeautifulSoup，soup.html是整个 DOM 树的根 Tag。\n",
    "\n",
    "学习 BeautifulSoup 主要就是学习这几种对象的使用。下面的代码展示了 soup 对象中不同节点的类型： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.NavigableString'>\n",
      "<class 'bs4.element.Comment'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print (type(soup))\n",
    "# soup.html 是 DOM 树的根 TAG\n",
    "print (type(soup.html) )\n",
    "# soup.title 是一个 TAG。soup.title.string 是 NavigableString，作为soup.title唯一的子节点，其中包含了<title>标签中的内容\n",
    "print (type(soup.title.string))\n",
    "# 同样，soup.comment 是一个 TAG。soup.comment.string 是 Comment，作为soup.comment唯一的子节点，其中包含了<comment>标签中的内容\n",
    "print (type(soup.comment.string))\n",
    "print (soup.comment.string is soup.html.body.comment.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的代码中就可以看出，soup.comment 是 soup.html.body.comment 的快捷方法，这也就是我们为什么可以把BeautifulSoup对象当做 Tag 对象使用的原因。soup.html.body.comment 更能体现出 HTML 中标签之间的包含关系，如<html>是顶级标签，<body>包含在<html>中，<comment>包含在<body>中。\n",
    "\n",
    "需要注意的一点是，BeautifulSoup 中文文本部分也是用 DOM 中的节点表示的，即 NavigableString。并且为了忠于原始 HTML 的内容， BeautifulSoup 不会对其中的空白部分做处理，每一段文本都会被当做一个NavigableString节点。我们知道，原始 HTML 文档的 <body> 标签中只有3个子标签：两个 div ，还有一个 comment。那么 soup.body 应该有三个子节点。真的这样么？请看下面的程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "<class 'bs4.element.NavigableString'> None 换行1\n",
      "  \n",
      "<class 'bs4.element.Tag'> div <div id=\"div1\">\n",
      "<h1>一级标题</h1>\n",
      "<a class=\"link1\" href=\"http://www.example.com/example1.html\">链接1</a>\n",
      "<a class=\"link2\" href=\"example2.html\">链接2</a></div>\n",
      "<class 'bs4.element.NavigableString'> None 换行2\n",
      "  \n",
      "<class 'bs4.element.Tag'> div <div id=\"div2\"><h2>二级标题</h2><ul><li value=\"1\">项目1</li><li>项目2</li></ul></div>\n",
      "<class 'bs4.element.NavigableString'> None 换行3\n",
      "  \n",
      "<class 'bs4.element.Tag'> comment <comment><!--This is comment，这是注释--></comment>\n",
      "<class 'bs4.element.NavigableString'> None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (len(soup.body.contents)) # tag.contents 可以获取 tag 的直接子节点的列表。tag.children 可以获取 tag 的直接子节点的生成器。\n",
    "for child in soup.body.contents:\n",
    "    print (type(child), child.name, child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码显示 <body> 竟然有7个子节点！这是为什么呢？从输出结果可以看到， soup.body下还有四个NavigableString类型的节点。每个NavigableString节点都代表一段夹在标签之间的文本，如\"换行1\\n\"，\"换行2\\n\"，\"换行3\\n\"，\"\\n\"，注意</comment>后面的一个换行符单独占用一个NavigableString节点。注意，NavigableString节点不是 Tag，所以 name 属性是 None，并且一定是叶节点，所以没有 contents 属性和 children 属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签定位\n",
    "\n",
    "经过上面的学习，相信大家对 BeautifulSoup 生成的 DOM 树的结构已经有了比较清楚的认识。剩下的就是从 DOM 树提取想要的数据了！ 数据提取的第一步，通常是定位到指定的标签，上面已经使用了 \"soup.\"+标签名字定位标签的方法。但这种定位方法只能选择第一个满足条件的节点。如 <body> 中有两个 div：div1 和 div2，但 soup.div 只能选择 div1。所以我们更常用的方法是 find_all 方法，这个方法能返回所有满足条件的标签的列表。\n",
    "\n",
    "我们可以按标签名称定位标签： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"link1\" href=\"http://www.example.com/example1.html\">链接1</a>, <a class=\"link2\" href=\"example2.html\">链接2</a>]\n",
      "[<a class=\"link1\" href=\"http://www.example.com/example1.html\">链接1</a>, <a class=\"link2\" href=\"example2.html\">链接2</a>, <li value=\"1\">项目1</li>, <li>项目2</li>]\n"
     ]
    }
   ],
   "source": [
    "print (soup.find_all('a')) # 选择所有标签名称为'a'的节点。\n",
    "print (soup.find_all(['a', 'li'])) # 选择所有标签名称为'a'或'li'的节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"link2\" href=\"example2.html\">链接2</a>]\n",
      "[<a class=\"link2\" href=\"example2.html\">链接2</a>]\n",
      "[<a class=\"link2\" href=\"example2.html\">链接2</a>]\n",
      "[<div id=\"div2\"><h2>二级标题</h2><ul><li value=\"1\">项目1</li><li>项目2</li></ul></div>]\n"
     ]
    }
   ],
   "source": [
    "print (soup.find_all('a', 'link2')) # 选择包含属性 class=\"link2\"的 'a' 标签。\n",
    "print (soup.find_all('a', class_='link2'))# 同上。注意，因为class 是 Python 中的关键字，所以利用 class 属性定位时，要用 class_。\n",
    "print (soup.find_all('a', attrs={'class':'link2'})) # 同上\n",
    "print (soup.find_all('div', id='div2')) # 选择包含属性 id='div2' 的 'div' 标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以按文本内容定位标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"link1\" href=\"http://www.example.com/example1.html\">链接1</a>]\n",
      "[<a class=\"link1\" href=\"http://www.example.com/example1.html\">链接1</a>, <a class=\"link2\" href=\"example2.html\">链接2</a>]\n"
     ]
    }
   ],
   "source": [
    "print (soup.find_all('a', text='链接1')) # 选择内容为 '链接1' 的 'a' 标签。另外，为了保证编码的一致性，此处最好使用 unicode 字符串。\n",
    "print (soup.find_all('a', text=['链接1','链接2'])) # 选择内容为 '链接1' 或 '链接2' 的 'a' 标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标签定位的方法基本就这么多，为了拥有更好地扩展性，find_all 方法还支持用正则表达式和自定义函数来过滤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"link1\" href=\"http://www.example.com/example1.html\">链接1</a>, <a class=\"link2\" href=\"example2.html\">链接2</a>, <li value=\"1\">项目1</li>, <li>项目2</li>]\n",
      "[<a class=\"link1\" href=\"http://www.example.com/example1.html\">链接1</a>, <a class=\"link2\" href=\"example2.html\">链接2</a>]\n",
      "[<a class=\"link1\" href=\"http://www.example.com/example1.html\">链接1</a>, <a class=\"link2\" href=\"example2.html\">链接2</a>]\n",
      "[<li value=\"1\">项目1</li>, <li>项目2</li>]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print (soup.find_all(re.compile(r'(^a$)|(^li$)'))) # 用正则表达式，选择所有标签名称为'a'或'li'的节点\n",
    "print (soup.find_all('a', class_=re.compile(r'link'))) # 用正则表达式，选择所有包含属性 'class'，并且'class'属性中包含 'link'的'a' 标签。\n",
    "\n",
    "def tag_filter(tag): # 自定义函数\n",
    "    return tag.has_attr('class')\n",
    "print (soup.find_all(tag_filter)) #选择所有包含 'class' 属性的标签。注意 find_all 方法可以只接受一个筛选条件。 \n",
    "\n",
    "def text_filter(text):\n",
    "    return '项目' in text\n",
    "print (soup.find_all('li', text=text_filter)) # 选择所有内容包含 '项目' 的 'li' 标签\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据提取\n",
    "\n",
    "定位标签之后，就要从标签中提取出想要的内容。有时，我们想要获取标签中的属性值：\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.example.com/example1.html']\n",
      "['http://www.example.com/example1.html']\n",
      "['http://www.example.com/example1.html']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tags = soup.find_all('a', href=re.compile(r'^http')) # 选择所有包含 'value' 属性，并且属性值以 'http' 开头的 'a' 标签。\n",
    "print ([tag['href'] for tag in tags] )# 获取 value 属性的值\n",
    "print ([tag.get('href') for tag in tags])# 同上\n",
    "print ([tag.attrs['href'] for tag in tags]) # 同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "链接1\t链接2\n",
      "链接1\t链接2\n",
      "换行1\t一级标题\t链接1\t链接2\t换行2\t二级标题\t项目1\t项目2\t换行3\n",
      "换行1\t一级标题\t链接1\t链接2\t换行2\t二级标题\t项目1\t项目2\t换行3\n"
     ]
    }
   ],
   "source": [
    "tags = soup.find_all('a') # 选择所有的 'a' 标签\n",
    "print ('\\t'.join([tag.text for tag in tags])) # 获取 'a' 标签包含的内容\n",
    "print ('\\t'.join([tag.get_text() for tag in tags]))# 同上\n",
    "print (soup.body.get_text('\\t', strip=True)) #获取 'body' 标签及其子标签中包含的所有内容（标签本身除外），删除多余的空白，并将各项用'\\t' 分隔开\n",
    "print ('\\t'.join(soup.body.stripped_strings)) # 首先获取'body' 标签及其子标签中包含的所有 NavigableString 节点（删除多余的空白），然后用'\\t' 分隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前为止，我们已经学会了 BeautifulSoup 对 HTML 的表示方式（DOM 树），以及标签的定位和内容的提取。是不是比正则表达式要简单明了多了！\n",
    "\n",
    "除了find_all方法，还有一个find方法，这个方法只返回一个结果，使用方法与 find_all 类似。推荐使用 find_all。\n",
    "\n",
    "另外，BeautifulSoup 中还包含了一些其它有用的方法，如：parents，next_sibling，find_parents，find_next_siblings等。这些方法从字面意思上也很容易理解，它们主要是用于遍历 DOM 树。前面我们讲解的内容主要是对 DOM 树进行查找，其实 BeautifulSoup 还提供了修改 DOM 树的方法，如append，insert，extract，replace_with等。这些方法固然有用，但对于面向数据分析的爬虫开发，我们通常是用不着的。所以我们就暂且跳过这些内容。\n",
    "\n",
    "\n",
    "可能你也会注意到如果不设置response的编码为UTF-8，就会导致乱码。前面讲到 requests 会自动推断网页的编码，对于 http://www.princetechs.com/ 这个网页，requests 却推断成了 ISO-8859-1。但这也不能全怪 requests 无能，根本原因还是http://www.princetechs.com/ 在设置网页编码的时候少了一个分号：<meta http-equiv=\"content-type\" content=\"text/html\" charset=\"UTF-8\">，requests 不得不根据字符推断编码，因为编码方式太多了，这很容易出错。只要在 charset 前面加上 ';'，requests 就能正确推断编码了。在浏览器中看起来正常，并不是因为浏览器能识别出编码，而是因为浏览器在不能识别编码的时候，采用默认的编码，通常是 UTF-8，歪打正着而已。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
